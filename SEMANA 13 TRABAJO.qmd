---
title: "PC4 SEMANA 13"
format: html
editor: visual
---

## CURSO: SISTEMATIZACION Y METODOS ESTADISTICOS

INTEGRANTES:

-Sánchez Gutiérrez Aracely Nathaly

-Salvador Aybar Valle

-César Augusto Aquije Hurtado

-Danuska Ascarza Aiquipa

## Cargar los paquetes

```{r}
library(factoextra)
library(cluster)
library(here)
library(rio)
library(tidyverse)
```

## 1 ¿Cómo aplicaremos Machine Learning a esta sesión?

En esta sesión, utilizaremos técnicas de Machine Learning no supervisado, como PCA y K-Means, para explorar patrones ocultos en pacientes según múltiples variables clínicas, sin usar directamente la etiqueta de diagnóstico (`diabetes_5a`).

Tu base contiene múltiples mediciones por paciente: glucosa, presión arterial, insulina, IMC, historial familiar, etc. Estas variables pueden estar correlacionadas y formar patrones comunes que no se ven a simple vista.

## 1.1 Uso de las técnicas de agrupamiento para responden preguntas de investigación en salud

Agrupar pacientes con perfiles parecidos sin usar el diagnóstico final (diabetes_5a), y luego:

Observar si algunos grupos presentan mayores niveles de riesgo (por ejemplo, altos niveles de glucosa, insulina y edad avanzada).

Comparar los grupos respecto a la proporción de pacientes con diagnóstico positivo de diabetes, incluso si no se usó esa variable para agrupar.

Formular hipótesis: ¿hay un subgrupo con factores clínicos silenciosos pero alto riesgo? ¿Hay perfiles saludables bien diferenciados?

## 2 Análisis de agrupamiento herarquico (Hierarchical Clustering)

## 2.1 Sobre el problema para esta sesión

El dataset de esta sesión contiene información clínica de pacientes que han sido evaluados para determinar su riesgo de desarrollar diabetes en un horizonte de cinco años. La base incluye variables numéricas como glucosa, presión arterial, insulina, índice de masa corporal, edad, antecedentes familiares de diabetes, entre otras. El objetivo de este ejercicio es aplicar métodos de agrupamiento no supervisado, como el análisis de componentes principales (PCA) y el algoritmo de K-Means, para identificar grupos de pacientes que compartan características clínicas similares. Esto permitirá explorar posibles perfiles de riesgo y patrones de salud diferenciados, que podrían ser útiles para la detección temprana o la orientación de intervenciones preventivas.

## 2.2 El dataset para esta sesión

El dataset diabetes.csv contiene información clínica de pacientes evaluados por riesgo de desarrollar diabetes. Incluye variables como número de embarazos, glucosa, presión arterial, pliegue cutáneo, insulina, IMC, historial familiar y edad. También registra si el paciente desarrolló diabetes en cinco años. Este conjunto permite aplicar técnicas de agrupamiento para identificar perfiles clínicos similares y explorar posibles grupos de riesgo.

### 2.2.1 Importando los datos

```{r}
diabetes_data <- import(here("data", "diabetes.csv"))
```

## 2.3 Preparación de los datos

### 2.3.1 Solo datos numéricos

Para el análisis de agrupamiento jerárquico de esta sesión usaremos solo variables numéricas. Es posible emplear variables categóricas en esta técnica, pero esto no será cubierto aquí. El código abajo elimina las variables categóricas `Sexo` y `Enfermedad_renal`. `id` será el identificador para los participantes.

```{r}
diabetes_data_1 <- diabetes_data |>
  mutate(id = row_number()) |>              # crea columna id
  select(-diabetes_5a) |>                   # elimina variable categórica
  column_to_rownames("id")                  # usa id como nombres de fila
```

### 2.3.2 La importancia de estandarizar

En tu dataset `diabetes.csv`, es importante estandarizar las variables antes de aplicar técnicas de agrupamiento, ya que cada una está medida en diferentes unidades y escalas. Variables como glucosa, IMC, insulina o edad tienen rangos distintos, lo que puede generar sesgos en el análisis. La estandarización con `scale()` en R transforma todas las variables para que tengan media cero y desviación estándar uno, permitiendo que todas contribuyan por igual al análisis.

```{r}
diabetes_data_escalado = scale(diabetes_data_1)
```

Un vistazo a los datos antes del escalamiento:

```{r}
head(diabetes_data_1)
```

y un vistazo después del escalamiento:

```{r}
head(diabetes_data_escalado)
```

**RESULTADOS:**

Los valores resultantes no se interpretan en sus unidades originales, sino como posiciones relativas respecto al promedio del conjunto de datos:

Un valor positivo indica que el paciente está por encima del promedio en esa variable.

Un valor negativo indica que está por debajo del promedio.

Un valor cercano a cero indica proximidad al promedio general.

Por ejemplo, en la primera observación escalada:

El paciente presenta una edad muy superior al promedio del conjunto (1.42 desviaciones estándar por encima).

Tiene valores de glucosa, IMC y pliegue cutáneo por encima del promedio.

Su puntaje de historial familiar de diabetes es inferior al promedio.

No cuenta con datos de insulina post carga, por lo que aparece como valor ausente (NA).

## 2.4 Cálculo de distancias

Dado que uno de los pasos es encontrar "cosas similares", necesitamos definir "similar" en términos de distancia. Esta distancia la calcularemos para cada par posible de objetos (participantes) en nuestro dataset. Por ejemplo, si tuvieramos a los pacientes A, B y C, las distancia se calcularían para A vs B; A vs C; y B vs C. En R, podemos utilizar la función `dist()` para calcular la distancia entre cada par de objetos en un conjunto de datos. El resultado de este cálculo se conoce como matriz de distancias o de disimilitud.

```{r}
dist_diabetes_data <- dist(diabetes_data_escalado, method = "euclidean")
```

## 2.4.1 (opcional) Visualizando las distancias euclidianas con un mapa de calor

Una forma de visualizar si existen patrones de agrupamiento es usando mapas de calor (heatmaps). En R usamos la función `fviz_dist()` del paquete factoextra para crear un mapa de calor.

```{r}
fviz_dist(dist_diabetes_data)
```

**RESULTADOS:**

La visualización fue generada mediante la función fviz_dist() del paquete factoextra, que produce un mapa de calor donde:

Cada celda representa la distancia entre dos pacientes.

Colores azules oscuros indican mayor similitud (distancias pequeñas).

Colores rosados o anaranjados indican mayor diferencia (distancias grandes).

En el gráfico, se pueden observar ciertas regiones más compactas de color azul o violeta, lo cual sugiere que algunos subconjuntos de pacientes presentan características clínicas más similares entre sí, es decir, potenciales grupos naturales dentro del conjunto de datos. Estas agrupaciones visuales respaldan la viabilidad de aplicar técnicas de clustering como el agrupamiento jerárquico o K-Means en los siguientes pasos.

## 2.5 El método de agrupamiento: función de enlace (linkage)

El agrupamiento jerárquico es un método que empieza agrupando las observaciones más parecidas entre sí, por lo que es fácil de usar al comienzo. Sin embargo, no basta con calcular las distancias entre todos los pares de objetos. Una vez que se forma un nuevo grupo (clúster), hay que decidir cómo medir la distancia entre ese grupo y los demás puntos o grupos ya existentes. Hay varias formas de hacerlo, y cada una genera un tipo diferente de agrupamiento jerárquico. La función de enlace (linkage) toma la información de distancias devuelta por la función `dist()` y agrupa pares de objetos en clústeres basándose en su similitud. Luego, estos nuevos clústeres formados se enlazan entre sí para crear clústeres más grandes. Este proceso se repite hasta que todos los objetos del conjunto de datos quedan agrupados en un único árbol jerárquico. Hay varios métodos para realizar este agrupamiento, incluyendo *Enlace máximo o completo*, *Enlace mínimo o simple*, *Enlace de la media o promedio*, *Enlace de centroide*, *Método de varianza mínima de Ward*. No entraremos en detalle sobre cómo funciona estos métodos, pero para este contexto el método de varianza minima de Ward o el método máximo, son preferidos. En este ejemplo, usamos el método de varianza mínima de Ward.

```{r}
dist_link_diabetes_data <- hclust(d = dist_diabetes_data, method = "ward.D2")
```

## 2.7 Dendrogramas para la visualización de patrones

Los dendrogramas es una representación gráfica del árbol jerárquico generado por la función `hclust()`.

```{r}
fviz_dend(dist_link_diabetes_data, cex = 1.0)
```

**RESULTADOS:**

Cada línea horizontal del gráfico indica la unión de dos grupos o individuos, y la altura en la que ocurre dicha unión refleja la distancia o disimilitud entre ellos. Uniones más bajas indican mayor similitud, mientras que uniones más altas indican que los grupos fusionados son más diferentes entre sí.

En el dendrograma generado, se observan varias ramas bien definidas, lo que sugiere la existencia de estructuras de agrupamiento claras en los datos. Estos patrones refuerzan la hipótesis de que existen perfiles clínicos distintos dentro del conjunto de pacientes analizado.

```{r}
fviz_dend(dist_link_diabetes_data,
          k = 3,
          cex = 0.5,
          k_colors = c("#00CFFF", "#8E44AD", "#FFD700"),
          color_labels_by_k = TRUE,
          rect = TRUE
)
```

**RESULTADOS:**

A partir del dendrograma generado mediante el método de agrupamiento jerárquico con enlace de Ward (ward.D2), se aplicó un corte para formar tres grupos principales. Esta decisión se basa en la estructura visual del árbol, donde se observan tres ramas grandes claramente diferenciadas en términos de altura de fusión.

El gráfico muestra estos grupos destacados con diferentes colores:

Celeste agrupa pacientes con perfiles clínicos similares entre sí y distintos a los otros dos grupos.

Morado representa un segundo grupo con características intermedias o mixtas.

Amarillo ( incluye un tercer conjunto de pacientes, posiblemente con un patrón clínico diferente o más homogéneo dentro del grupo.

# 3 Agrupamiento con el algoritmo K-Means

El método de agrupamiento (usando el algoritmo) K-means es la técnica de machine learning más utilizado para dividir un conjunto de datos en un número determinado de k grupos (es decir, k clústeres), donde k representa el número de grupos predefinido por el investigador. Esto contrasta con la técnica anterior, dado que aquí sí iniciamos con un grupo pre-definido cuya idoniedad (de los grupos) puede ser evaluado. En detalle, el esta técnica clasifica a los objetos (participantes) del dataset en múltiples grupos, de manera que los objetos dentro de un mismo clúster sean lo más similares posible entre sí (alta similitud intragrupo), mientras que los objetos de diferentes clústeres sean lo más diferentes posible entre ellos (baja similitud intergrupo). En el agrupamiento k-means, cada clúster se representa por su centro (centroide), que corresponde al promedio de los puntos asignados a dicho clúster.

Aquí como funciona el algoritmo de K-Means

1.  Indicar cuántos grupos (clústeres) se quieren formar. Por ejemplo, si se desea dividir a los pacientes en 3 grupos según sus características clínicas, entonces K=3.
2.  Elegir aleatoriamente K casos del conjunto de datos como centros iniciales. Por ejemplo, R selecciona al azar 3 pacientes cuyas características (edad, IMC, creatinina, etc.) servirán como punto de partida para definir los grupos.
3.  Asignar cada paciente al grupo cuyo centro esté más cerca, usando la distancia euclidiana. Es como medir con una regla cuál centroide (paciente promedio) está más próximo a cada paciente en función de todas sus variables.
4.  Calcular un nuevo centro para cada grupo. Es decir, calcular el promedio de todas las variables de los pacientes que quedaron en ese grupo. Por ejemplo, si en el grupo 1 quedaron 40 pacientes, el nuevo centroide será el promedio de la edad, IMC, creatinina, etc., de esos 40 pacientes. Este centroide es un conjunto de valores (uno por cada variable).
5.  Repetir los pasos 3 y 4 hasta que los pacientes dejen de cambiar de grupo o hasta alcanzar un número máximo de repeticiones (en R, por defecto son 10 repeticiones). Esto permitirá que los grupos finales sean estables.

## 3.1 El problema y dataset para este ejercicio

Usaremos el mismo dataset y el mismo problema que el que empleamos en el ejercicio anterior (para Agrupamiento Jerárquico).

## 3.2 Estimando el número óptimo de clusters

Como indiqué arriba, el método de agrupamiento k-means requiere que el usuario especifique el número de clústeres (grupos) a generar. Una pregunta fundamental es: ¿cómo elegir el número adecuado de clústeres esperados (k)?

Aquí muestro una solución sencilla y popular: realizar el agrupamiento k-means probando diferentes valores de k (número de clústeres). Luego, se grafica la suma de cuadrados dentro de los clústeres (WSS) en función del número de clústeres. En R, podemos usar la función fviz_nbclust() para estimar el número óptimo de clústeres.

Primero escalamos los datos:

```{r}
diabetes_data_escalado = scale(diabetes_data_1)
```

```{r}
diabetes_data_escalado_sin_na <- diabetes_data_escalado |>
  na.omit()
```

```{r}
fviz_nbclust(diabetes_data_escalado_sin_na, kmeans, nstart = 25, method = "wss") +
  geom_vline(xintercept = 3, linetype = 2)
```

**RESULTADOS:**

Para estimar el número adecuado de grupos en el análisis de K-means, se utilizó el método del codo (elbow method). Esta técnica evalúa la suma de cuadrados intra-cluster (WSS) para diferentes valores de k (cantidad de clusters), mostrando cómo varía la compacidad de los grupos conforme aumenta k

En el gráfico se observa una fuerte disminución inicial de la WSS entre 1 y 3 clusters, lo que indica una ganancia significativa de homogeneidad al subdividir la muestra en hasta tres grupos. A partir de k = 3, la pendiente de la curva se aplana, lo cual sugiere que añadir más clusters genera beneficios marginales reducidos

Este punto de inflexión se conoce como el "codo" de la curva, y corresponde al valor óptimo de k según este criterio. En este caso, se identifica k = 3 como el número óptimo de clusters, lo cual concuerda con los resultados obtenidos previamente mediante el dendrograma jerárquico.

Esta coincidencia respalda la validez de segmentar la población en tres perfiles clínicos distintos, que serán caracterizados en los siguientes pasos.

## 3.3 Cálculo del agrupamiento k-means

Dado que el resultado final del agrupamiento k-means es sensible a las asignaciones aleatorias iniciales, se especifica el argumento `nstart = 25`. Esto significa que R intentará 25 asignaciones aleatorias diferentes y seleccionará la mejor solución, es decir, aquella con la menor variación dentro de los clústeres. El valor predeterminado de `nstart` en R es 1. Sin embargo, se recomienda ampliamente utilizar un valor alto, como 25 o 50, para obtener un resultado más estable y confiable. El valor empleado aquí, fue usado para determinar el número de clústeres óptimos.

```{r}
set.seed(123)
km_res <- kmeans(diabetes_data_escalado_sin_na, 3, nstart = 25)
```

```{r}
km_res
```

**RESULTADOS:**

Cada cluster presenta un conjunto de valores promedio (centróides) para las variables clínicas estandarizadas, lo que permite caracterizar a los grupos:

Cluster 1: presenta valores promedio moderadamente altos en insulina, pliegue tricipital e historial familiar, pero edad por debajo del promedio. Podría representar un perfil metabólico activo en pacientes relativamente más jóvenes

Cluster 2: muestra los valores más altos en glucosa, presión, insulina y edad, lo que sugiere un perfil clínico de alto riesgo. Podría agrupar a los pacientes con mayor probabilidad de desarrollar diabetes o con una evolución clínica más avanzada

Cluster 3: destaca por valores por debajo del promedio en la mayoría de variables, especialmente en glucosa, pliegue tricipital, insulina y presión. Representa un grupo de pacientes clínicamente más estables o menos comprometidos

## 3.4 Visualización de los clústeres k-means

Al igual que el análisis anterior, los datos se pueden representar en un gráfico de dispersión, coloreando cada observación o paciente según el clúster al que pertenece. El problema es que los datos contienen más de dos variables, y surge la pregunta de qué variables elegir para representar en los ejes X e Y del gráfico. Una solución es reducir la cantidad de dimensiones aplicando un algoritmo de reducción de dimensiones, como el Análisis de Componentes Principales (PCA). El PCA transforma las 52 variables originales en dos nuevas variables (componentes principales) que pueden usarse para construir el gráfico.

La función `fviz_cluster()` del paquete factoextra se puede usar para visualizar los clústeres generados por k-means. Esta función toma como argumentos los resultados del k-means y los datos originales (hemo_data_escalado).

```{r}
fviz_cluster(
  km_res,
  data = diabetes_data_escalado_sin_na,
  palette = c("#00CFFF", "#8E44AD", "#FFD700"),
  ellipse.type = "euclid",
  repel = TRUE,
  ggtheme = theme_minimal()
)
```

**RESULTADOS:**

La figura muestra el resultado del algoritmo k-means proyectado en un espacio bidimensional usando componentes principales (Dim1 y Dim2), que explican conjuntamente una proporción importante de la variabilidad total en los datos (Dim1: 32%, Dim2: 19.5%).

Cada punto representa a un paciente, y los colores indican el grupo asignado por el modelo:

Cluster 1 (celeste): agrupa una gran cantidad de pacientes dispersos a lo largo de Dim1, con valores más altos en variables asociadas al primer componente principal.

Cluster 2 (morado): forma un grupo denso y claramente diferenciado hacia la parte derecha del gráfico. Su compactación sugiere alta homogeneidad interna.

Cluster 3 (dorado): también forma un grupo definido, separado espacialmente de los otros, aunque con mayor dispersión a lo largo del eje vertical (Dim2).
