---
title: "Regresi√≥n Lineal Simple y prueba t de Student"
grupo: 9

participantes:
Danuska Yamile Ascarza Aiquipa
SANCHEZ GUTIERREZ ARACELY NATHALY
AYBAR VALLE SALVADOR JESUS
AQUIJE HURTADO CESAR AUGUSTO
format: html
editor: visual
---

# Cargar e instalar paquetes

```{r}
install.packages("car") # Para la prueba de Levene
```

```{r}
library(tidyverse)
library(here)
library(rio)
library(gtsummary)
library(car)
library(ggplot2)
```

# Cargando los datos

```{r}
circun_glucosa <- import(here("data","diabetes.csv"))
```

# Sobre los datos para esta pr√°ctica

El dataset circun_glucosa, de 1000 personas adultas (\>=20 a√±os de edad), contiene datos glucosa medida en ayunas (en mg/dL), cirunferencia de cintura (en centimetros), tabaquismo y otros datos demogr√°ficos.

```{r}
names(diabetes)
```

## 1.1 El problema en este ejercicio

El desenlace *Y* de inter√©s para este ejercicio es la variable glucosa medida en ayunas. Veamos la distribuci√≥n de la variable y el promedio en en un histograma.

```{r}
diabetes |>  
  ggplot(aes(x = glucosa_2h)) +
  geom_histogram(
    color = "white",
    ) + 
  labs(y = "Frecuencia", 
       x = "resultados") +
  geom_vline(xintercept = mean(diabetes$glucosa_2h, na.rm = TRUE),
             color = "darkred", size = 1.5)
```

En estos datos, el promedio de la glucosa es:

```{r}
mean(diabetes$glucosa_2h, na.rm = TRUE)
```

Una observaci√≥n importante a partir del histograma y el promedio (el valor esperado) es que existe una gran variaci√≥n entre los valores de glucosa de los individuos de quienes provienen los datos. Podemos hipotetizar de que otras variables (predictores) podr√≠an influir en esta variaci√≥n, por ejemplo, la circunferencia de cintura.

## 1.2 Notaci√≥n en el m√©todo de regresi√≥n lineal simple

```{r}
plot(glucosa_2h ~ insulina_2h , data = diabetes,
     col = "gray",
     ylab = "resultado",
     xlab = "edad",
     las = 1,
     pch = 20, 
     font.lab = 2, font.axis = 2) 

# La funci√≥n lm() ajusta el modelo de regresi√≥n lineal
abline(lm(glucosa_2h ~ insulina_2h , data = diabetes), lwd = 2, col = "yellow")
```

La ecuaci√≥n siguiente ecuaci√≥n describe un modelo de regresi√≥n lineal simple para ùëå usando un predictor continuo ùëã. $$
Y = \beta_0 + \beta_1 X + \epsilon
$$ Cuando ajustamos un modelo de regresi√≥n lineal simple a nuestros datos, estimamos (hallamos) los par√°metros del modelo que mejor explican la relaci√≥n entre las dos variables (desenlace y predictor), incluyendo los coeficientes (Œ≤‚ÇÄ, Œ≤‚ÇÅ) y el error (ùúÄ), que representa la variabilidad no explicada por el modelo.

Para un predictor continuo, el intercepto (Œ≤‚ÇÄ) es el valor esperado de Y cuando X = 0 (es decir, el promedio del resultado cuando el predictor es cero). La pendiente (Œ≤‚ÇÅ) es el cambio promedio en Y por cada unidad de cambio en X. El t√©rmino de error (ùúÄ) representa la diferencia entre los valores observados y los valores predichos por el modelo.

Aplicado a nuestro ejemplo, el intercepto (Œ≤‚ÇÄ) representa la circunferencia de cintura promedio cuando la glucosa en ayunas es cero (aunque este valor puede no tener sentido pr√°ctico, es necesario matem√°ticamente). La pendiente (Œ≤‚ÇÅ) indica cu√°nto aumenta (o disminuye) en promedio la circunferencia de la cintura por cada unidad adicional de glucosa en ayunas (medida en mg/dL). El error (ùúÄ) recoge la variaci√≥n individual que no es explicada solo por la glucosa.

Asi que, como el objetivo es hallar los valores de los par√°metros (Œ≤‚ÇÄ,Œ≤‚ÇÅ,ùúÄ), es apropiado decir que estamos 'ajustando el modelo de regresi√≥n lineal simple' para el problema planteado (a.k.a la asociaci√≥n entre glucosa y la circunferencia de cintura)

## 1.3 Ajustando el modelo de regresi√≥n lineal simple para nuestro problema

En R, usamos la funci√≥n lm() para ajustar un modelo de regresi√≥n lineal. "lm" es la abreviatura para "linear model". Dentro de la funci√≥n debemos indicarle como argumentos el desenlace X, el predictor Y y la data donde se encuentran las variables. Esta es la estructura para ajustar el modelo con la funci√≥n lm: lm(y \~ x, data = mis_datos).

Ajustando el modelo para nuestros datos

```{r}
modelo_ejemplo = lm(glucosa_2h ~ insulina_2h, data = diabetes)
```

Para ver los resultados, usamos la funci√≥n summary() y dentro, el objeto modelo_ejemplo.

```{r}
summary(modelo_ejemplo)
```

## 1.4 Interpretando los resultados

La secci√≥n Coefficients del resultado:

```{r}
summary(modelo_ejemplo)$coef
```

...muestra las estimaciones y las pruebas de hip√≥tesis para el intercepto (Œ≤‚ÇÄ), etiquetado como (Intercept), y para el coeficiente de la circunferencia de cintura (la pendiente, Œ≤‚ÇÅ), etiquetado como Circunfe_brazo_cm.

En esta misma secci√≥n, la columna Estimate muestra los coeficientes estimados del modelo de regresi√≥n lineal simple. As√≠, el modelo que mejor se ajusta tiene un intercepto de 59.474 y una pendiente de 0.49970.

La tabla de coeficientes tambi√©n muestra el error est√°ndar de cada estimaci√≥n, su valor t y su valor p (etiquetado como Pr(\>\|t\|)). El valor p del intercepto usualmente no es de inter√©s, pero el valor p del predictor (Circunfe_brazo_cm) prueba la hip√≥tesis nula de que el desenlace NO tiene asociaci√≥n con el predictor o, dicho de otra manera, que la pendiente es cero. La hip√≥tesis nula plantea que la l√≠nea de mejor ajuste es una l√≠nea horizontal, lo que indicar√≠a que el promedio esperado del desenlace es el mismo en todos los valores del predictor; es decir, que no existe asociaci√≥n entre el desenlace (glucosa) y el predictor (circunferencia de cintura).

Finalmente, el valor R-cuadrado es una medida de bondad de ajuste que var√≠a entre 0 (sin asociaci√≥n) y 1 (asociaci√≥n lineal perfecta), y corresponde al cuadrado de la correlaci√≥n de Pearson entre el desenlace y el predictor. Se interpreta como la proporci√≥n de la variaci√≥n en el desenlace que es explicada por el modelo. En nuestro modelo, el R¬≤ (R-cuadrado) es 0.0871. Esto significa que aproximadamente el 8.6% de la variaci√≥n en los valores de glucosa en ayunas se explica por la circunferencia de la cintura

## 1.5 ¬øC√≥mo reportar los resultados del ajuste del modelo de regresi√≥n lineal simple?

Tanto si se trata de una tesis o un art√≠culo, abajo un ejemplo de c√≥mo reportar los resultados del presente problema:

Adicionalmente, es buena idea presentar los resultados en un tabla.

```{r}
theme_gtsummary_language("es")

tabla_reporte <- modelo_ejemplo |> 
  tbl_regression(intercept = T, estimate_fun = function(x) style_sigfig(x, digits = 4),
                 pvalue_fun   = function(x) style_pvalue(x, digits = 3),
                 label = list(insulina_2h ~ "insulina")) |>
  modify_caption("resultado insulina")

tabla_reporte
```

**Exportamos la tabla**

```{r}
tabla_reporte |> 
  as_flex_table()  |> 
  flextable::save_as_docx(path = "tabla_reporte.docx")
```

# 2 Prueba t de Student para muestras independientes

Imagina que, ahora, luego de haber tomado las mediciones de medidas de glucosa en ayunas (mg/dL) queremos saber si el promedio de glucosa en varones es significativamente diferente del promedio de glucosa en mujeres. Es esta situaci√≥n, hay dos grupos (varones y mujeres) de muestras independientes.

## 2.1 ¬øCu√°ndo usar la prueba t de Student para muestras independientes?

-   Cuando los dos grupos de muestras a comparar han sido muestreadas de una distribuci√≥n normal. Aqu√≠ podemos usar la prueba de Shapiro-Wilk.

-   Cuando las varianzas de los dos grupos son iguales. Esto puede ser evaluado con la prueba F.

Usualmente, la hip√≥tesis de la prueba t de Student son:

-   Hip√≥tesis nula (H‚ÇÄ): No hay diferencia entre las medias de los dos grupos. $$
    H_0: \mu_1 = \mu_2
    $$
-   Hip√≥tesis alternativa (H‚ÇÅ): Hay una diferencia entre las medias de los dos grupos. $$
    H_1: \mu_1 \neq \mu_2
    $$

## 2.2 Sobre los datos para esta pr√°ctica

El dataset circun_glucosa, de 1000 personas adultas (\>=20 a√±os de edad), contiene datos circunferencia de cintura (en cent√≠metros), la variable sexo y otros datos demogr√°ficos.

## 2.3 Resumen y visualizaci√≥n

Resumen

Antes de realizar la prueba t de Student es importante conocer la distribuci√≥n de los datos e identificar si hay valores perdidos o at√≠picos. Empecemos por el resumen:

```{r}
group_by(diabetes, diabetes_5a) |> 
  summarise(
    count = n(),
    mean = mean(insulina_2h, na.rm = TRUE),
    sd = sd(glucosa_2h, na.rm = TRUE)
  )
```

Visualizaci√≥n

```{r}
diabetes |>  
  filter(!is.na(diabetes_5a)& !is.na(insulina_2h))  |> 
  ggplot(aes(y = diabetes_5a, x = insulina_2h)) +
  geom_boxplot() +
  labs(y = "insulina", x = "diabetes")
```

## 2.4 Pruebas preliminares para evaluar los supuestos de la prueba t de Student

Supuesto 1: los datos deben haber sido muestreados de una distribuci√≥n normal.

Para esto, usamos la prueba de Shapiro-wilk.

```{r}
diabetes |> 
  filter(diabetes_5a == "negativo") |> 
  summarise(shapiro = list(shapiro.test(imc))) |> 
  pull(shapiro)
```

```{r}
diabetes |> 
  filter(diabetes_5a == "positivo") |> 
  summarise(shapiro = list(shapiro.test(imc))) |> 
  pull(shapiro)
```

Supuesto 2: Las varianzas de los dos grupos son iguales Para esto podemos usar la prueba F para evaluar la homogeneidad de varianzas. Esto esta implementado en la funci√≥n var.test()

```{r}
var.test(insulina_2h ~ diabetes_5a, data = diabetes)
```

El valor p de la prueba F es p = 0.3143. Es mayor que el nivel de significancia Œ± = 0.05. En conclusi√≥n, no hay una diferencia significativa entre las varianzas de los dos conjuntos (femenino y masculino) de datos. Por lo tanto, podemos usar la prueba t cl√°sica que asume igualdad de varianzas.

## 2.5 Realizamos la prueba t para nuestros datos.

```{r}
t.test(insulina_2h ~ diabetes_5a, data = diabetes, var.equal = TRUE)
```

**Interpretando los resultados**

El valor p de la prueba es 0.003615, lo cual es menor que el nivel de significancia Œ± = 0.05. Por lo tanto, podemos concluir que la circunferencia promedio del brazo en hombres es significativamente diferente de la circunferencia promedio en mujeres.

# 3 An√°lisis de Varianza (ANOVA)

El an√°lisis de varianza (ANOVA), especificamente el ANOVA de una v√≠a, es una extensi√≥n de la prueba t para muestras independientes cuando se comparan medias entre m√°s de dos grupos. En el ANOVA de una v√≠a, los datos se organizan en varios grupos basados en una √∫nica variable de agrupaci√≥n (tambi√©n llamada variable de factor). En este ejecicio, evaluamos si el peso corporal (kg) es significativamente distinto segun estado de tabaquismo.

## 3.1 ¬øCu√°ndo usar el ANOVA de una v√≠a?

-   Las observaciones se obtienen de forma independiente y aleatoria de la poblaci√≥n definida por los niveles del factor.

-   Los datos de cada nivel del factor se distribuyen normalmente.

-   Hip√≥tesis nula (H‚ÇÄ): No hay diferencia entre las medias de los dos grupos.

-   Estas poblaciones normales tienen una varianza com√∫n. (Se puede usar la prueba de Levene para verificar esto.)

## 3.2 Sobre los datos para esta pr√°ctica

El dataset circun_glucosa, de 1000 personas adultas (\>=20 a√±os de edad), contiene datos de peso corpotal (kg), la variable tabaquismo y otros datos demogr√°ficos.

## 3.3 Resumen y visualizaci√≥n

Resumen

Antes de realizar la prueba de ANOVA es importante conocer la distribuci√≥n de los datos e identificar si hay at√≠picos. Empecemos por el resumen:

```{r}
group_by(diabetes, insulina_2h) |> 
  summarise(
    count = n(),
    mean = mean(insulina_2h, na.rm = TRUE),
    sd = sd(insulina_2h, na.rm = TRUE),
    min = min(insulina_2h, na.rm = TRUE),
    max = max(insulina_2h, na.rm = TRUE)
  )
```

Visualizaci√≥n

```{r}
diabetes |>  
  filter(!is.na(diabetes_5a)& !is.na(edad))  |> 
  ggplot(aes(y = diabetes_5a, x = edad)) +
  geom_boxplot() +
  labs(y = "RESULTADO", x = "EDADES")
```

## 3.4 Pruebas preliminares para evaluar los supuestos del ANOVA

```{r}
diabetes <- diabetes |>
    mutate(diabetes_5a = as.factor(diabetes_5a))
```

Supuesto 1v: los datos deben haber sido muestreados de una distribuci√≥n normal.

Para esto, usamos la prueba de Shapiro-wilk.

```{r}
diabetes |> 
  filter(diabetes_5a == "positivo") |> 
  summarise(shapiro = list(shapiro.test(historial_diabetes))) |>
  pull(shapiro)
```

```{r}
diabetes |> 
  filter(diabetes_5a == "negativo") |> 
  summarise(shapiro = list(shapiro.test(imc))) |> 
  pull(shapiro)
```

```{r}
diabetes |> 
  filter(diabetes_5a == "positivo") |> 
  summarise(shapiro = list(shapiro.test(glucosa_2h))) |> 
  pull(shapiro)
```

```{r}
leveneTest(glucosa_2h ~ edad, data = diabetes)
```

## 3.5 Realizamos la prueba de ANOVA de una v√≠a para nuestros datos.

```{r}
res_anova = aov(glucosa_2h ~ edad, data = diabetes)
```

```{r}
summary(res_anova)
```

**Interpretando los resultados**

Dado que el valor p es mayor que el nivel de significancia 0.05, podemos concluir que no existen diferencias significativas entre los grupos.

Aunque para este ejecicio no hemos encontrado una diferencia estad√≠sticamente significativa, cuando s√≠ lo hay, es importante realizar una prueba de comparaci√≥n por pares para saber d√≥nde se encuentra la diferencia. Para esto, se puede utilizar la prueba Tukey HSD (Tukey Honest Significant Differences)

```{r}
TukeyHSD(res_anova)
```
